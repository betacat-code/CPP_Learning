# 第一章 虚拟化CPU
## 1.1 进程
进程就是运行中的程序。程序本身只是磁盘上的一些代码和静态数据，是操作系统把其加载到内存中并设置好内存和CPU使得程序可以运行。我们的终极目标是让多个程序并发运行，这就需要虚拟化CPU，机制是分时复用CPU。要实现分时复用CPU，就要实现进程的切换，即上下文切换。另外还需要有调度算法来确定接下来切换到哪一个进程来运行。

程序的指令和静态数据最初都在磁盘上，首先需要将其读入内存中，但现代操作系统惰性（lazily）执行该过程，即仅在程序执行期间需要加载的代码或数据片段，才会加载。

将代码和静态数据加载到内存后，操作系统在运行此进程之前还需要执行其他一些操作。必须为程序的运行时栈（run-time stack 或 stack）分配一些内存。程序使用栈存放局部变量、函数参数和返回地址。操作系统分配这些内存，并提供给进程。

操作系统也可能会用参数初始化栈。具体来说，它会将参数填入 main()函数，即 argc 和 argv数组。

操作系统也可能为程序的堆（heap）分配一些内存。在 C 程序中，堆用于显式请求的动态分配数据。程序通过调用 malloc()来请求这样的空间，并通过调用 free()来明确地释放
它。

## 1.2 机制：受限直接执行

直接执行部分很简单：只需直接在CPU上运行程序即可。因此，当OS希望启动程序运行时，它会在进程列表中为其创建一个进程条目，为其分配一些内存，将程序代码（从磁盘）加载到内存中，找到入口点（main()函数或类似的），跳转到那里，并开始运行用户的代码。
这种方法在我们虚拟化CPU时产生了一些问题。

- 第一个问题：如果我们只运行一个程序，操作系统怎么能确保程序只做我们允许它做的事情，同时仍然高效地运行它？

- 第二个问题：当我们运行一个进程时，操作系统如何让它停下来并切换到另一个进程，从而实现虚拟化CPU所需的时分共享？

### 对于问题1：
引入一种新的处理器模式，称为用户模式（user mode）。在用户模式下运行的代码会受到限制。例如，在用户模式下运行时，进程不能发出I/O请求。这样做会导致处理器引发异常，操作系统可能会终止进程。与用户模式不同的内核模式（kernel mode），操作系统（或内核）就以这种模式运行。在此模式下，运行的代码可以做任何它喜欢的事，包括特权操作，如发出I/O请求和执行所有类型的受限指令。

要执行系统调用，程序必须执行特殊的陷阱（trap）指令,该指令同时跳入内核并将特权级别提升到内核模式。一旦进入内核，系统就可以执行任何需要的特权操作（如果允许），从而为调用进程执行所需的工作。完成后，操作系统调用一个特殊的从陷阱返回（return-from-trap）指令

### 对于问题2：操作系统如何重新获取CPU的控制权？

利用时钟中断重新获得控制权 时钟设备可以编程为每隔几毫秒产生一次中断。产生中断时，当前正在运行的进程停止，操作系统中预先配置的中断处理程序（interrupt handler）会运行。此时，操作系统重新获得CPU的控制权。

### 保存和恢复上下文
为当前正在执行的进程保存一些寄存器的值（保存到它的内核栈），并为即将执行的进程恢复一些寄存器的值（从它的内核栈取出）。

这样一来，操作系统就可以确保最后执行从陷阱返回指令时，不是返回到之前运行的进程，而是继续执行另一个进程。

## 1.3 进程调度
- 周转时间：一个进程从提交到运行结束的总时间
- 相应时间：一个进程从提交到首次开始运行的时间

### 多级反馈队列：MLFQ

其尝试用历史预测未来。它在运行过程中，根据已经观察到的行为来调整进程的优先级等，从而利用反馈的信息来适应当前情况。
- 设置多个不同优先级的队列，优先运行高优先级队列中的进程。
- 进程提交到系统后，首先放在最高优先级队列中。一个队列中采用轮询调度(Round-Robin)算法来运行。
- 每个队列都有一个时间配额属性，一个进程用完了这个队列规定的时间后就要降到低一级队列。
- 每经过一段时间就要把所有进程重新加入最高级队列。

对于短进程，其可以在较高优先级的队列中较快完成。对于长进程，周期性地把所有进程重新加入最高级队列使其不至于饿死。并且对于不同时间段行为不同的进程，也可以根据其行为来合理地调度它。

MLFQ依然有一些问题，最大的问题是如何配置MLFQ。设置多少队列？每一个队列的RR算法的时间片以及时间配额是多少？提高所有进程优先级的周期是多少？这些问题没有确切答案，可能做就是在运行过程中动态调整来取得平衡。但是让系统自己去学习一个好的参数也不容易，因此我们会有一个包含默认值的配置文件，并且用户可以给出建议从而利于OS动态调整参数。

## 1.4 多处理器调度

多核处理器由于有多个CPU核，而每个CPU都有自己的cache，因此存在一个多核CPU的cache一致性问题。基本解决方法是使用总线，每个CPU都监听其他CPU和内存的操作，如果发现对自己缓存中同一内存地址的数据的更新，就作废cache中的数据（valid设为0）或者直接进行更新。

另外对于多核CPU，并发问题是绕不开的，需要加锁解锁，但高锁争用会导致并行性很差。

cache亲和性：一个进程在一个CPU核上运行时，会在该CPU的cache、TLB、分支预测器等硬件上维护很多状态。如果把一个进程尽量调度到同一个CPU核上，会由于这些缓存中的数据从而运行更快。多处理器调度应该考虑到cache亲和性。

调度策略上，最简单的是复用单核调度：为所有CPU核维护一个队列即可，每个CPU核空闲时都从队列头取出一个进程来运行。其最大优点是简单，但是由于多核并行访问因此需要锁来保证原子性，这就会带来性能损失。还有问题就是cache亲和性，每个CPU核都简单地从队列中取出一个进程来运行，因此每个进程都可能会在不同CPU间迁移，从而不利于cache亲和性。

可以采用多队列调度，为每个CPU维护一个队列。这样锁争用和cache亲和性都不再是问题，但会产生一个问题就是工作负载的分配。例如一个新提交的进程应该加入哪个队列，以及更重要的是如果两个队列的工作负载情况差异较大，应该怎么样实现负载均衡？目前无共识答案。

# 第二章 虚拟化内存

## 2.1 地址转换
在虚拟化CPU时使用的机制是受限直接访问。就是让用户程序的大部分指令直接使用硬件CPU和内存，仅在一些关键时间点时OS介入。

那么在虚拟化内存时使用的机制是地址转换。用户级程序中的地址全是其虚拟地址空间中的虚拟地址，其在访存时会被专门硬件和操作系统合作翻译成物理地址，从而得到内容。

CPU 需要两个硬件寄存器：基址（base）寄存器和界限（bound）寄存器，有时称为限制（limit）寄存器。界限寄存器提供了访问保护。如果进程需要访问超过这个界限或者为负数的虚拟地址，CPU 将触发异常，进程最终可能被终止。

<code>
physical address = virtual address + base
</code>

## 2.2 分段

如何支持稀疏地址空间？

泛化基址+界限这个概念，为地址空间中的每个段设置一个基址+界限。比如代码、堆、栈段。这样已使用的内存才会真正分配物理内存。

硬件在地址转换时使用段寄存器。它如何知道段内的偏移量，以及地址引用了哪个段？

- 显式方式，就是用虚拟地址的开头几位来标识不同的段
- 隐式方式中，硬件通过地址产生的方式来确定段

分段也带来了一些新的问题：物理内存很快充满了许多空闲空间的小洞，因而很难分配给新的段，或扩大已有的段。这种问题被称为外部碎片。

## 2.3 分页

分段会产生外部碎片，因此可以考虑分页，把虚拟地址空间分割成固定大小的单元，每个单元就是页。而物理内存中的一个页称为一个页帧。为了追踪每个页对应的物理页帧，OS为每个进程维护一个页表。

虚拟地址分为虚拟页号VPN和页内偏移offset，地址翻译的流程就是根据页表查找VPN(virtual page number)对应的物理页号PPN(physical page number)，找到对应的物理页帧，并根据offset找到所需字节。页表比较巨大，因此我们将其存在物理内存中，而不是内存中。

页表就是一种数据结构，用于将虚拟地址（或者实际上，
是虚拟页号）映射到物理地址（物理帧号）最简单的形式称为线性页表，就是一个数组。操作系统通过虚拟页号检索该数组，并在该索引处查找页表项，以便找到期望的物理帧号。

页表项（PTE）中还有一些标志位、例如有效位、保护位rwx、访问位、修改位等。

## 2.4 快速地址转换:TLB

TLB缓存最近使用的VPN到PPN的映射。TLB作为一种缓存，其基本访问原理和cache一样，TLB命中就直接得到PPN，未命中就需要访问页表得到PPN并更新TLB。其能有效的原理是局部性。

TLB作为一种缓存，其是全相联的。因为TLB未命中的开销很大，要采用全相联来尽可能命中。主要策略有LRU和随机。

TLB在进程切换时会有问题，上一个进程的TLB内容对于下一个进程来说是无意义的。为此可以把TLB全部有效位都置为0。但是上一个进程再次运行时性能就会降低，在频繁进程切换之下总体性能会很低。

为了减少这种开销，一些系统增加了硬件支持，实现跨上下文切换的 TLB 共享。有了地址空间标识符，TLB 可以同时缓存不同进程的地址空间映射。

## 2.5 多级页表

多级页表的基本思想：将页表分成页大小的单元。如果整页的页表项（PTE）无效，就完全不分配该页的页表。为了追踪页表的页是否有效（以及如果有效，它在内存中的位置），使用了名为页目录（page directory）的新结构。

多级页表大大节省了空间，但其并非没有缺点，实际上它是一个时间——空间的折中。TLB未命中时，访存次数相比线性页表还会增加。

>在构建数据结构时，应始终考虑时间和空间的折中
(time-space trade-off)。通常，如果你希望更快地访问特定的数据结构，就必须为该结构付出空间的代价。

使用多级页表的地址翻译会将原来的VPN拆成几个小的VPN，分别用于在不同级页表中索引PTE。最高级页表的物理地址由OS为每个进程在一个寄存器中维护，进程切换时会保存并恢复该寄存器以切换地址空间。

## 2.6 虚拟内存

使用物理内存+磁盘为进程支持巨大的虚拟内存空间。页可以在内存和磁盘间交换。之前如果一个进程产生对应的PTE无效的虚拟地址，这个进程也许会被直接kill。

但现在，进程访问的可能是不在物理内存中而是在磁盘中的页。这样产生了page fault，进程会trap进内核中的page fault handler来处理缺页。OS会根据PTE中存储的磁盘地址信息来发起磁盘I/O，然后更新PTE中的PPN和有效位，为了方便起见同时也会更新TLB中的PPN和有效位，最后重新执行产生page fault的指令。如果访问不合法的页就会提示“段错误”，进程被直接kill。

在真实系统中，空闲页帧数少于某个警戒线时，后台的一个页守护进程就会运行，驱逐出一定数目的页帧。由于磁盘I/O的开销非常大，因此我们需要尽可能避免缺页。

近似LRU是现代OS采用的方法。硬件为每个页维护一个访问位，通常在这个页对应的PTE的标志位中。这个页被访问时（读或写）硬件将其置1.只有OS才可以将其置0。

每个页都在一个逻辑上的环形列表里，指针最初始指向某个页，当开始进行页替换时，开始扫描每个页的访问位，若其为1，说明最近被访问过，不适合驱逐，将其置为0，扫描下一个，直到扫描到一个本来就是0的，将其驱逐。

这被称为CLOCK算法。其可以考虑页是否被写入过，利用PTE中的脏位，优先驱逐没有被写入过的。注意可能会扫描到访问位为0而修改位为1的情况，这种页是之前访问过，访问位为1，但是被上一次CLOCK算法扫描之后设为了0，并且在两次扫描之间没有被访问过。

页面驱逐优先级是：(访问位，修改位)，(0，0)>(0，1)>(1，0)>(1，1)。

页面替换只是虚拟内存中的一个策略。对应的还有页面读取，通常使用请求分页，即一个页面真正被访问时才加载进内存，并且可以预取页面。以及将页面写入磁盘的策略，一次写一个可以但效率低，通常分组写入效率更高。

# 第三章 并发
## 3.1 多线程

线程是轻量级的进程，多线程程序会有多个执行点（多个程序计数器，每个都用于取指令和执行），共享地址空间。

线程运行顺序具有不确定性，它们在访问共享数据时可能会产生一些错误结果。其根源是进程调度不可控，并发的不确定性。不可控的调度会导致线程以错误顺序访问临界区。

对临界区的访问应该是互斥的，不能由多线程同时执行。解决这个问题实际上是确保对临界区访问的原子性，防止执行一部分而破坏一致性。可以用锁来解决，进入临界区前必须持有对应的锁，执行完之后释放锁，锁在某一时刻最多只能被一个进程持有。这是互斥问题。

多线程中还有同步问题，即一个线程需要等待另一个线程完成某些操作才能继续执行。我们可以使用条件变量来实现wait/signal机制，线程之间可以通过条件变量来通信。而信号量是一种较强的机制，互斥和同步它都可以解决。

## 3.2 锁

锁保证了临界区的原子性。锁让程序员获得一些线程调度的控制权，可以保证临界区内只有一个活跃线程。mutex的意义就是互斥。

<code>lock()</code>和 <code>unlock()</code>函数的语义很简单。调用 <code>lock()</code>尝试获取锁，如果没有其他线程持有锁，该线程会获得锁，进入临界区。这个线程有时被称为锁的持有者。如果另外一个线程对相同的锁变量调用 <code>lock()</code>，因为锁被另一线程持有，该调用不会返回。这样，当持有锁的线程在临界区时，其他线程就无法进入临界区。


锁的持有者一旦调用 <code>unlock()</code>，锁就变成可用了。如果没有其他等待线程（即没有其他线程调用过 <code>lock()</code>并卡在那里），锁的状态就变成可用了。如果有等待线程（卡在 <code>lock()</code>里），其中一个会（最终）注意到（或收到通知）锁状态的变化，获取该锁，进入临界区。

## 3.3 条件变量

很多情况下线程需要等到某一条件满足后才能继续运行。若不满足就简单自旋会浪费时间片，我们期望让它休眠，进入阻塞态，直到等待的条件满足后进入就绪态。如何等待？我们可以使用条件变量来提供支持的同步协议。它有wait和signal两个操作。一个条件变量和一个锁相关联。

一个线程通常会先持有锁，通过while而不是if来检查状态变量是否满足，若不满足就使用wait在这个条件变量上sleep。成功调用wait会首先释放锁，直到状态满足，线程wake时会重新持有锁，进行后续操作并释放锁。调用wait和signal时都要持有条件变量的锁，并且一个状态变量是有必要的，这都是为了防止一个线程可能无法被唤醒。

    int done = 0;
    pthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER;
    pthread_cond_t c = PTHREAD_COND_INITIALIZER;
    void thr_exit() {
        Pthread_mutex_lock(&m);
        done = 1;
        Pthread_cond_signal(&c);
        Pthread_mutex_unlock(&m);
    }
    void *child(void *arg) {
        printf("child\n");
        thr_exit();//退出
        return NULL;
    }
    void thr_join() {
        Pthread_mutex_lock(&m);
        while (done == 0)//必须通过while
            Pthread_cond_wait(&c, &m);
        Pthread_mutex_unlock(&m);
    }

## 3.4 信号量
信号量是有一个整数值的对象，可以用两个函数来操作它。在 POSIX 标准中，是<code>sem_wait()</code>和 <code>sem_post()</code>

    include <semaphore.h>
    sem_t s;
    sem_init(&s, 0, 1);
    
其中申明了一个信号量 s，通过第三个参数，将它的值初始化为 1。第二个参数，在我们看到的所有例子中都设置为 0，表示信号量是在同一进程的多个线程共享的。

首先，<code>sem_wait()</code>要么立刻返回（调用时，信号量的值大于等于 1），要么会让调用线程挂起，直到之后的一个 post 操作。可能多个调用线程都调用 <code>sem_wait()</code>，因此都在队列中等待被唤醒。

<code>sem_post()</code>并没有等待某些条件满足。它直接增加信号量的值，如果有等待线程，唤醒其中一个。最后，当信号量的值为负数时，这个值就是等待线程的个数。

## 3.5 死锁

并发编程容易出问题。比如未使用锁导致临界区的不互斥访问，违反了原子性，又如未使用条件变量导致线程间的不恰当时序关系。实际上绝大多数bug都是这两种。

此外就是死锁，如果要同时获得多个锁，就要小心死锁。死锁的产生需要四个必要条件：

- 互斥访问，线程对资源的访问是互斥的
- 持有并等待，线程持有某些资源，然后继续等待其他所需资源
- 循环等待，线程的等待链条形成一个环
- 非抢占，不能抢占其他线程的资源

死锁预防：破坏死锁的四个必要条件之一。最常用的是避免循环等待，规定抢占锁的顺序，但要对锁的作用有深入了解。然后是避免持有并等待，线程一下子获得所有资源后才开始运行，但会降低并发性和资源利用率。接着是避免非抢占，就是线程可以去抢其他线程的资源，但可能会产生线程互相抢来抢去一个资源从而导致活锁。最后就是避免互斥，这个是最难的。

>实际系统中都是采用最简单的，允许死锁发生，当发生时我们可以采取一些行动来处理（例如重启），或者是干脆就不管。很多数据库系统使用了死锁检测和恢复技术。死锁检测器会定期运行，通过构建资源图来检查循环。

## 3.6 基于事件的并发


多线程只是用来实现并发的一种方式。在GUI应用和某些服务器中使用的是event-based concurrency，例如Node.js。多线程中，并发的正确实现非常有难度，且对于并发线程的调度无法控制。

事件驱动的并发就是等待某些事件的发生（例如点击鼠标、网络数据包到来），将其添加进队列，然后主程序就处理这些事件。这样调度就完全可控，且一次处理一个事件，不需要锁。得知某些事件的发生，即接收事件，可以使用Linux的select或poll系统调用。

但有一个重要问题是我们不能阻塞主程序，例如发起磁盘I/O。在多线程中这不是问题，但在事件驱动中我们没有其他线程，只是main event的loop。一个阻塞的调用就会阻塞整个服务器，这会造成资源的极大浪费和性能的极大损失。

为此我们可以使用异步I/O，其相当于只是发送一个请求I/O的信号，若发送成功之后会立即返回去处理其他事件。如果不使用异步I/O，纯事件驱动的方法是无法工作的。但也可以采取一些混合方法，如使用线程池来管理I/O。

# 第四章 I/O设备

典型计算机系统的架构采用分层：性能越高就离CPU越近，并且可以在外围总线上连接大量设备。

状态寄存器可以让OS查看设备状态。命令寄存器可以让OS命令设备执行某种任务。数据寄存器让OS和设备间交互数据。要设计交互协议，可以首先轮询状态寄存器直到就绪，然后写数据并发送指令，然后再轮询状态寄存器直到其执行完了指令。这样简单正确但低效。

可以使用中断来代替轮询，发起I/O时向设备发送一个请求，完成时设备触发中断。这样可以显著提高CPU和设备的并行性。但如果是一个非常高性能的设备，一次询问就可以返回，那么中断的处理和进程切换反而会造成浪费。

如果设备的速度是未知的，可以采用混合(hybrid)的策略：先轮询一会，如果设备还是没有完成任务，就进入中断。

另外在网络中不宜使用中断，若每个数据包都发生一次中断，那么负载会过大，OS发生活锁。web服务器应优先服务用户请求，周期轮询网卡。多次中断还可以合并成一次来降低中断处理的代价。

在中断中，传输的所有数据都需要经过CPU在内存和外设间传递，这样CPU负载很重。因此可以使用DMA，CPU告诉DMA源数据的信息和传输位置，DMA就可以自主工作，并在任务全部完成后触发中断，这样将CPU从繁重的数据传输中解放了出来。

# 第五章 文件系统

## 5.1 文件系统API

磁盘用于持久存储数据，文件系统就是管理持久数据的软件，即管理和抽象硬件磁盘。文件就是字节数组，目录是一种特殊的文件。每个文件都关联一个inode，以及inode number。

一个比较重要的抽象是文件描述符fd。open一个文件返回的就是fd，它是一个small int，每个进程私有。实际上它是每个进程的ofile（打开文件列表）的下标，可以看成是哈希表。进程操作这些文件仅通过fd就行了。

write和read没有指定文件的偏移量。因此OS为每个打开文件维护一个偏移量，也许是在内存中的inode结构体里。write和read会隐式改变这个偏移量，还可以调用lseek显式更改，但注意其和磁盘寻道没有直接关联。

调用write时只是用户告诉FS让它写入数据，但FS会将这些数据在内存中缓冲一段时间再进行I/O。这就有可能造成数据丢失。DBMS需要FS提供一个立即强制写入磁盘的原语，来设计其故障恢复协议。可以调用fsync强制立刻将缓冲区中数据写入磁盘，写入完成后返回。

可以使用rename来原子性重命名文件，这在文本编辑器中很有用，因为其通常是创建一个临时文件并写入，确定成功后用rename将其更改为本来的名称，并删除旧版本文件。

目录是一种特殊的文件，但用户不能直接更改这个文件的数据，而是通过在其下创建或删除文件或子目录，通过FS间接更改。使用mkdir可创建空目录，其包括.和..两个目录项，通过rmdir删除空目录。ls程序的作用就是读取目录.

对于普通文件的删除调用unlink。为此我们需要先理解link。它为一个文件创建了另一个名称，实际上创建了硬链接。硬链接实际上是添加了一个新目录项，该目录项的inode号与旧目录项相同，即对同一个inode创建了新的引用。创建一个文件需要创建其inode和dentry。使用unlink会删除对应的dentry，并将inode的引用计数减一，若减到零则删除inode。stat的输出中就包含inode的引用计数。硬链接的局限是不能创建目录的硬链接（以防在目录树中形成环），以及不能跨磁盘分区创建硬链接（inode号在每个文件系统中是独立的）。

区别于硬链接，符号链接本身就是一个类型是"symbol link"的文件。这个文件的数据就是指向的文件的路径名。因此删除指向的文件会造成悬空引用。

## 5.2 FSDI—文件系统设计与实现

FS操纵磁盘的基本单位是多个连续扇区组成的块。首先，绝大多数块都应该是用户数据块。但FS需要有能力并且记录文件信息等元数据。

我们需要为inode分配区域。并且FS有能力管理这些块，因此需要块的分配和管理机制。可以采用类似物理内存的freelist链表结构，但bitmap更简单且流行，我们为inode和数据块维护bitmap。

最后我们还需要超级块，它记录FS的元数据，例如bitmap、inode和数据区的起始处和长度，以及一个magic number来标识文件系统类型。mount文件系统时，OS会首先读取超级块，初始化一些in-memory的参数，将其添加进现有目录树。

每个文件对应一个inode，其具有唯一的inode号，根据该inode号可在磁盘上找到该inode。每个inode大小固定，在知道inode区域的起始地址之后，就可以根据inode号和inode大小算出该inode的字节地址，并读出其所在块以得到该inode。

>  关键问题是根据一个文件的inode怎么找到文件的数据块？


每个inode维护一个文件的所有元数据。可以在inode中包含多个直接指针（磁盘地址）。但这样可能只能找到几十KB的数据。为此可以引入间接指针，根据要引用的数据块的所在地址范围来根据一系列直接和间接指针引用它。

另一种方法是基址加界限，由一个磁盘地址和一个范围就可以确定一个文件的数据块，但容易产生外部碎片。基于指针的方法会产生大量元数据，基于范围不灵活但更加紧凑。

使用这种不平衡树状的多级索引支持很大的文件。文件系统中的大多数文件很小，仅用直接指针就可以全部引用到，因此对小文件优化是有意义的。

此外文件的数据块还可以组织成链式结构，并在内存中维护文件分配表来支持随机访问，其是哈希表，使用块号查找其下一个块。

至于目录，其在逻辑上就是一系列目录项组成的，但使用的数据结构可以是线性表也可以是B树（目录项的访问较快）等。

空闲块的管理，可以使用freelist空闲块链表，可以使用bitmap，可以使用B树，有很多方法。

## 5.3 文件系统性能和可靠性

FFS（快速文件系统）的核心思想是：磁盘就是磁盘。FS要知道它管理的是磁盘，要针对磁盘的一些特性做出优化。磁盘不是内存，随机访问的性能要远低于顺序访问。

考虑读取文件的最基本操作——先读取inode再根据指针读取数据块。若数据块离inode很远，开销就会增大很多。文件系统最终可能变得碎片化，逻辑上的一个文件要来回访问，我们的目标应该是尽可能地多顺序访问。

FFS将磁盘划分为柱面组，或“块组”。每个块组里都相当于是一个小型文件系统，拥有超级块、位图、inode、数据块。FFS的原则就是把相关的东西放在同一个块组内。因此它需要决定什么是 “相关的” 。

例如在块组间平衡目录的数量，将目录项和对应inode放在同一个块组中，将一个文件的inode和数据块放在同一块组中。因为同一个目录中的文件常常一起访问，这样的块组安排可以减少相关文件之间的寻道时间。

对于大文件，如果占据其所在的块组太多内容则会影响其他相关文件的存放。FFS采用了简洁的策略：把直接块放到inode所在块组中，后续的间接块以及指向的所有块都在不同块组中，即前48KB和后续每4MB放在同一块组中（假设的大小）。

## 5.4 可靠性——使用fsck
文件系统的目标就是持久存储数据，但持久性不是那么容易实现的。如果在写入磁盘的过程中OS突然崩溃或突然断电，FS可能就会进入不一致状态。想想之前我们描述的读写文件过程中FS进行的那么多操作，如果只进行了一部分就崩溃掉了，FS的状态将会是什么？

考虑写入一个文件，我们要更新inode、更新数据位图和写入数据块。考虑原子性被破坏会出现的可能结果：

- 若只成功更新inode，则其中会有新数据块的指针。此时该数据块在位图中没有分配，但inode却指向了它。这种就是典型的不一致状态，我们根据inode的指针会读取到垃圾数据，并且之后很可能会被覆写而得到新的垃圾数据。
- 若只成功更新数据位图，对应的数据块已分配，却没有inode指向它，我们永远没法使用它了。这会造成存储空间泄漏。
- 若只成功写入数据块，inode和位图的状态一致，用户的数据却丢失了。
- 若inode和位图成功，这两者状态是一致的，但指向的数据块中依然是垃圾数据。
- 若inode和数据块成功，我们可以根据inode使用写入的数据，但位图中其是空闲的，之后很可能会被覆写而产生错误。
- 若位图和数据块成功，则没有inode指向它，会造成存储空间泄漏。


fsck（文件系统检查程序）允许不一致状态的发生，它在运行时检查不一致状态并修复。它检查的是元数据的一致性，如果inode和位图状态一致但inode指向的是垃圾数据，它是无法修复的。它通常在挂载文件系统时运行，检查完成后交给用户使用。

## 5.5 日志

在磁盘上划分出日志区，任何更新磁盘的操作之前都需要先写日志，即记录要做的更新。这样崩溃之后我们就可以通过查看日志来知道怎么恢复了，而不用扫描整个磁盘。

一旦日志写完，我们就可以开始真正的更新。这个过程叫加检查点，即让FS的状态与日志一致。如果真正的更新完成了，就是成功加上了检查点。即我们的更新分为：写入日志、加检查点。

> 如果写入日志时崩溃了怎么办？

可以利用磁盘的硬件特性——对单个512B的扇区的写入是原子性的。因此我们可将日志结束标志设计成只有512B大小。这样更新就可以分成：

- 日志写入，在日志区域中写入日志开始标志和日志内容。
- 日志提交，把日志结束标志写入日志。由于它的写入是原子的，日志只会有完全成功提交和完全失败两种状态。加检查点。

保证了日志的可靠性（要么完全成功提交，要么完全失败，作废）

> 如何根据日志来恢复文件系统？

对于未提交的日志，直接跳过；对于已提交的日志则进行redo，保证FS与日志的一致性。如果redo时发生崩溃，那么下次进行故障恢复时依然会进行redo，最差的情况就是几次冗余的redo而已。

日志将故障恢复的时间复杂度从O(磁盘分区大小)降到了O(日志大小)，并对日志又采取了很多优化，因此许多现代FS采取日志机制。

一个比较流行的机制是写时复制。它不会覆写数据块，而是在空闲块中创建原始块的一个副本进行写入并更改指针使其指向副本。

## 5.6 分布式文件系统缓存一致性问题

想象一下客户端 C1读取文件 F，并将文件的副本保存在其本地缓存中。现在假设一个不同的客户端 C2 覆盖文件 F，从而改变其内容。我们称该文件的新版本为 F（版本 2），或 F [v2]，称旧版本为 F [v1]，以便区分两者。最后，还有第三个客户端 C3，尚未访问文件 F。



第一个子问题是，客户端 C2 可能将它的写入缓存一段时间，然后再将它们发送给服务器。在这种情况下，当F[v2]位于 C2 的内存中时，来自另一个客户端（比如 C3）的任何对 F 的访问，都会获得旧版本的文件（F[v1]）。

缓存一致性的第二个子问题是陈旧的缓存（stale cache）。在这种情况下，C2 最终将它的写入发送给文件服务器，因此服务器具有最新版本（F[v2]）。但是，C1 的缓存中仍然是F[v1]。如果运行在 C1 上的程序读了文件 F，它将获得过时的版本（F[v1]），而不是最新的版本（F [v2]）

为了解决更新可见性，客户端实现了“关闭时刷新”（flush-on-close，即 close-to-open）的一致性语义。当应用程序写入文件并随后关闭文件时，客户端将所有更新（即缓存中的脏页面）刷新到服务器。通过关闭时刷新的一致性，NFS 可确保后续从另一个节点打开文件，会看到最新的文件版本。

为了解决陈旧的缓存问题，NFSv2 客户端会先检查文件是否已更改，然后再使
用其缓存内容。具体来说，在打开文件时，客户端文件系统会发出 GETATTR 请求，以获取文件的属性。重要的是，属性包含有关服务器上次修改文件的信息。如果文件修改的时间晚于文件提取到客户端缓存的时间，则客户端会让文件无效（invalidate），因此将它从客户端缓存中删除，并确保后续的读取将转向服务器，取得该文件的最新版本。另外，如果客户端看到它持有该文件的最新版本，就会继续使用缓存的内容。