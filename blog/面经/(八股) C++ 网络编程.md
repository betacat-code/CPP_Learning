# 阻塞和非阻塞IO

当应用程序发起一个IO操作（如读取文件或从网络套接字接收数据）时，程序会被阻塞，直到IO操作完成为止。在阻塞IO模式下，程序在进行IO操作时会暂时停止执行，直到IO操作完成，然后再继续执行后续的代码。

当应用程序发起一个IO操作时，程序会立即返回并继续执行后续代码，而不会等待IO操作完成。在非阻塞IO模式下，程序会周期性地轮询IO操作的状态，以确定是否已经完成。


# I/O多路复用

I/O多路复用是一种允许单个进程监视多个文件描述符以等待一个或多个文件描述符上的I/O可用的技术。常见的I/O多路复用技术包括select、poll和epoll（仅在Linux上）。


## 流程

准备：应用程序告诉内核监视哪些文件描述符（sockets、文件等）以及监视哪种类型的I/O事件（可读、可写等）。

阻塞等待：应用程序通过调用select、poll或epoll_wait等函数阻塞（等待），直到一个或多个文件描述符上发生了感兴趣的I/O事件。

事件通知：当一个或多个文件描述符上发生了I/O事件，或者超时（如果指定了超时时间）时，调用返回。返回后，应用程序可以得知哪些文件描述符上发生了哪些事件。

事件处理：应用程序根据事件通知结果，执行相应的I/O操作（如读取、写入等），无需担心操作会阻塞，因为已知有事件发生。

循环：应用程序通常会回到步骤1，更新需要监视的文件描述符和事件类型，然后再次调用相应的函数阻塞等待。


## 原理

通过操作系统提供的特定系统调用（如select、poll、epoll（Linux专有）等）来实现的。这些系统调用允许应用程序注册一组I/O流，并等待这些流中的任何一个变得可用于非阻塞I/O操作。

注册I/O流：应用程序使用select/poll/epoll等系统调用，指定它想要监视的一组文件描述符（FDs），以及它对每个文件描述符感兴趣的事件类型（如可读、可写、错误等）。

等待事件：应用程序在调用select/poll/epoll_wait时阻塞，直到以下情况之一发生：

一个或多个监视的文件描述符上发生了应用程序指定的事件；
- 调用超时（如果指定了超时时间）。
- 事件通知：当一个或多个文件描述符准备好进行I/O操作或调用超时时，系统调用返回。应用程序随后可以检查哪些文件描述符上发生了哪些事件。

## 实现机制

select：维护一个文件描述符集合，每次调用时都需要将整个集合从用户空间传到内核空间。

poll：与select类似，但使用动态空间代替固定大小的集合。

epoll：使用一个事件通知机制，只需将感兴趣的事件注册在红黑树上（用于存储所有被监视的文件描述符及其事件）。epoll通过在内核中维护一个就绪队列来避免每次调用时复制大量数据，提高了大规模并发处理的效率。

# epoll

epoll是Linux内核提供的一种高效I/O事件通知机制，主要用于处理大量的并发连接和事件。以下是使用epoll的主要原因：

高效性：epoll使用了一种基于事件驱动的I/O模型，避免了频繁的系统调用（如select和poll），从而提高了系统的性能和吞吐量。

节省资源：epoll使用一个文件描述符来管理多个连接，而不是每个连接都需要一个文件描述符，从而节省了资源。

支持大量并发连接：epoll没有连接数的限制，可以处理成千上万个并发连接，非常适合于高并发场景，如Web服务器、消息中间件等。

支持多种触发模式：epoll支持水平触发（LT）和边缘触发（ET）两种模式，可以根据应用的需求选择不同的触发模式。这使得开发者可以更加灵活地处理I/O事件。

数据拷贝轻量：epoll只在合适的时候调用EPOLL_CTL_ADD将文件描述符结构拷贝到内核中，这个操作并不频繁，相比select和poll每次循环都要进行拷贝，epoll的数据拷贝更为轻量。

## 实现原理

数据结构：
- rb_root rbr，这是红黑树的根节点，存储着所有添加到 epoll 中的事件，也就是这个 epoll 监控的事件。
- list_head rdllist 这是一个双向链表，保存着将要通过 epoll_wait 返回给用户的、满足条件的事件。


epoll的操作：调用 epoll_create 建立一个 epoll 对象（在 epoll 文件系统中给这个句柄分配资源）、调用 epoll_ctl 向 epoll 对象中添加连接的套接字、调用 epoll_wait 收集发生事件的连接。

当进程调用 epoll_create 方法时，内核会创建一个 eventpoll 对象，也就是应用程序中的 epfd（epoll 文件描述符） 所代表的对象。eventpoll 对象也是文件系统中的一员，和socket一样也有一个等待队列。

创建epoll对象 eventpoll 之后，可以使用 epoll_ctl 添加或者删除所要监听的socket。内核会将eventpoll添加到需要监听的socket的等待队列中。当socket收到数据后，中断回调程序会操作eventpoll对象，而不是直接操作进程。

在 eventpoll 对象中存在就绪列表，rdlist（双向链表 保存着将要通过 epoll_wait 返回给用户满足条件的事件）。中断回调程序会给eventpoll的就绪列表添加socket的引用。eventpoll对象相当于是socket和进程之间的中介，socket的数据接收并不直接影响进程，而是通过改变eventpoll的就绪列表来改变进程状态。

epoll_wait的返回条件也是根据rdlist的状态进行判断：如果rdlist已经引用了socket，那么epoll_wait直接返回（把发生的事件的集合从内核复制到 events数组中）；如果rdlist为空，阻塞进程。

（对于epoll，操作系统只需要将进程放入eventpoll这一个对象的等待队列中；而对于select，操作系统则需要将进程放入到socket列表中的所有socket对象的等待队列中。）

## 工作模式

LT(level triggered) 是默认/缺省的工作方式，同时支持 block和no_block socket。这种工作方式下，内核会通知你一个fd是否就绪，然后才可以对这个就绪的fd进行I/O操作。就算没有任何操作，系统还是会继续提示fd已经就绪，这种工作方式出错会比较小，传统的select/poll就是这种工作方式的代表。

ET(edge-triggered) 是高速工作方式，仅支持no_block socket，这种工作方式下，当fd从未就绪变为就绪时，内核会通知fd已经就绪，并且内核认为你知道该fd已经就绪，不会再次通知了，除非因为某些操作导致fd就绪状态发生变化。如果一直不对这个fd进行I/O操作，导致fd变为未就绪时，内核同样不会发送更多的通知。所以这种方式下，出错率比较高，需要增加一些检测程序。

LT可以理解为水平触发，只要有数据可以读，不管怎样都会通知。而ET为边缘触发，只有状态发生变化时才会通知，可以理解为电平变化。

设想这样一个场景，当一次read()读取没有读取完缓冲区中的数据时，LT和ET的区别：

- LT，此时缓冲区中还有数据，会继续发通知
- ET，此时缓冲区状态并没有发生变化，并没有来新的数据，就不会发通知，在新数据到来之前，之前剩余的数据就无法取出。



## 底层为什么用红黑树不用hash


红黑树容易缩容，在处理完大规模数据后能够很好的缩容。哈希表是不容易缩容的，如果某个时刻，有大量的网络请求通过哈希来记录，触发哈希表扩容，之后这个哈希表很难缩容回去。

红黑树是一种自平衡二叉查找树，它的查询、插入和删除操作的平均复杂度都是O(log n)。而哈希表的查询、插入和删除操作的平均复杂度是O(1)，在处理一些小规模的数据时，哈希表可能表现得更出色，但在处理大规模的数据时，红黑树更加高效稳定。



# poll

poll 使用链表结构来表示文件描述符的状态，没有最大连接数的限制。

poll的实现和select非常相似，只是描述fd集合的方式不同，poll使用pollfd结构而不是select的fd_set结构，poll不限制socket描述符的个数，因为它是使用链表维护这些socket描述符的，其他的都差不多和select()函数一样，poll()函数返回后，需要轮询pollfd来获取就绪的描述符，根据描述符的状态进行处理，但是poll没有最大文件描述符数量的限制。

poll和select同样存在一个缺点就是，包含大量文件描述符的数组被整体复制于用户态和内核的地址空间之间，而不论这些文件描述符是否就绪，它的开销随着文件描述符数量的增加而线性增大。

# select

select是一种IO多路复用技术

## 底层原理

数据结构：select 使用了三个位图数据结构来表示文件描述符的状态：读集合、写集合和异常集合。这三个集合分别用来标记哪些文件描述符处于可读、可写或异常状态。

select会维护一个文件描述符列表fd_set，用来存放需要监听的文件描述符fd，其本质是一个1024bit的bitmap数组，1代表需要检测的fd，0代表不需要检测的fd，初始时bitmap的元素全为1

用户进程调用select()函数，如果当前没有可读写的socket，则用户进程进入阻塞状态。

对于内核空间来说，它会从用户空间拷贝fd_set到内核空间，然后在内核中遍历一遍所有的socket描述符，如果没有满足条件的socket描述符，内核将进行休眠，当设备驱动发生自身资源可读写后，会唤醒其等待队列上睡眠的内核进程，即在socket可读写时唤醒，或者在超时后唤醒。

返回select()函数的调用结果给用户进程，返回就绪socket描述符的数目，超时返回0，出错返回-1。

**注意：**在select()函数返回后还是需要轮询去找到就绪的socket描述符的，此时用户进程才可以去操作socket。

## 缺点

每次调用select，都需要把fd集合从用户态拷贝到内核态，这个开销在fd很多时会很大。

同时每次调用select都需要在内核遍历传递进来的所有fd，这个开销在fd很多时也很大。

每次在select()函数返回后，都要通过遍历文件描述符来获取已经就绪的socket。

select支持的文件描述符数量太小了，默认是1024。

文件描述符集合不能重用，因为每次监听都会对该集合进行修改


## select为什么只能支持1024个？

因为它使用了一个固定大小的位图来表示文件描述符的状态。这个位图的大小通常是由操作系统定义的常量决定的，例如 FD_SETSIZE。默认情况下，FD_SETSIZE 在许多系统上都被设置为 1024，因此 select 只能监视 1024 个文件描述符。

poll 使用一个动态分配的数组来保存文件描述符的状态。在调用 poll 函数时，需要提供一个指向 pollfd 结构体数组的指针，数组大小可以根据应用程序的需要动态分配。

epoll 在内核中维护了一个红黑树，用于快速查找文件描述符的状态，而且它还可以通过调整内核参数来提高文件描述符的数量上限。

# Reactor模型

Reactor模型是一种事件驱动的设计模式，用于处理服务请求，它是由一个或多个并发输入源同时发送给一个事件处理器的模型。这个事件处理器是单线程的，它负责监听输入源（如：socket连接）上的事件，当事件发生时，它使用相应的回调函数来处理这些事件。

Reactor: 负责监听和分配事件, 将IO事件分派给对应的Handler, 新的事件包括连接建立就绪、读就绪、写就绪等

Acceptor: 处理客户端新连接, 并分派请求到处理器中

Handler: 将自身与事件绑定, 执行非阻塞IO任务, 完成channel读写, 一级业务逻辑

## 主从Reactor多线程模型

主Reactor监听到ACCEPT事件发生，表示此时有客户端建立连接；

主Reactor将ACCEPT事件分发给Acceptor处理；

Acceptor会在服务端创建与客户端通信的client-socket管道，然后注册到从Reactor的IO多路复用器selector上，并监听READ事件；

从Reactor监听到READ事件发生，表示此时客户端数据可读；

从Reactor将ACCEPT事件分发给Handler处理，Handler处理READ事件就会基于client-socket管道完成客户端数据的读取。

# 如何创建HTTPserver？

